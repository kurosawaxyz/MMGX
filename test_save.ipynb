{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My library\n",
    "from molgraph.dataset import *\n",
    "from molgraph.graphmodel import *\n",
    "from molgraph.training import *\n",
    "from molgraph.testing import *\n",
    "from molgraph.visualize import *\n",
    "from molgraph.interpret import *\n",
    "from molgraph.experiment import *\n",
    "# General library\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "# pytorch\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "args = parser.getArgument('''\n",
    "--file bbbp\n",
    "--model GIN\n",
    "--schema AR_0\n",
    "--reduced pharmacophore\n",
    "--vocab_len 100\n",
    "--mol_embedding 256\n",
    "--batch_normalize\n",
    "--fold 5\n",
    "'''.split())\n",
    "\n",
    "args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = args.file\n",
    "smiles = args.smiles \n",
    "task = args.task\n",
    "splitting = args.splitting \n",
    "splitting_fold = args.fold\n",
    "splitting_seed = args.splitting_seed\n",
    "\n",
    "# get validated dataset\n",
    "datasets = getDataset(file, smiles, task, splitting)\n",
    "# compute positive weight for classification\n",
    "if args.graphtask == 'classification':\n",
    "    args.pos_weight = getPosWeight(datasets)\n",
    "    print('pos_weight:', args.pos_weight)\n",
    "# generate dataset splitting\n",
    "datasets_splitted = generateDatasetSplitting(file, splitting, splitting_fold, splitting_seed)\n",
    "# generate all graph dataset\n",
    "datasets_graph = generateGraphDataset(file)\n",
    "# generate all reduced graph dataset\n",
    "dict_reducedgraph = dict()\n",
    "for g in args.reduced:\n",
    "    if g == 'substructure':\n",
    "        for i in range(splitting_fold):\n",
    "            vocab_file = file+'_'+str(i)\n",
    "            if not os.path.exists('vocab/'+vocab_file+'.txt'):\n",
    "                generateVocabTrain(file, splitting_seed, splitting_fold, vocab_len=args.vocab_len)\n",
    "            dict_reducedgraph[g] = generateReducedGraphDict(file, g, vocab_file=vocab_file)\n",
    "    else:\n",
    "        dict_reducedgraph[g] = generateReducedGraphDict(file, g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_test = dict()\n",
    "\n",
    "# Load model\n",
    "ts = \"2023-Apr-29-17:22:22\"\n",
    "reduced_list = '_'.join(args.reduced)\n",
    "args_test['log_folder_name'] = os.path.join(*[args.file, args.model+'_'+args.schema+'_'+reduced_list, f\"{ts}\"])\n",
    "args_test['exp_name'] = args.experiment_number\n",
    "args_test['fold_number'] = 1\n",
    "args_test['seed'] = args.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bin = torch.load('./dataset/'+args_test['log_folder_name']+'/checkpoints/training_args.bin')\n",
    "\n",
    "args.batch_size = training_bin.batch_size\n",
    "args.num_layers = training_bin.num_layers\n",
    "args.num_layers_reduced = training_bin.num_layers_reduced\n",
    "args.in_channels = training_bin.in_channels\n",
    "args.hidden_channels = training_bin.hidden_channels\n",
    "args.out_channels = training_bin.out_channels\n",
    "args.edge_dim = training_bin.edge_dim\n",
    "args.num_layers_self = training_bin.num_layers_self\n",
    "args.num_layers_self_reduced = training_bin.num_layers_self_reduced\n",
    "args.dropout = training_bin.dropout\n",
    "args.lr = training_bin.lr\n",
    "args.weight_decay = training_bin.weight_decay\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test with dataset\n",
    "# # test_loader, datasets_test =  generateDataLoaderTesting(datasets_graph[1], args.batch_size)\n",
    "# # test_loader, datasets_test =  generateDataLoaderTesting([datasets_graph[1][360]], 1)\n",
    "# test_loader, datasets_test =  generateDataLoaderTesting(args.file, 1)\n",
    "# sample_to_test = datasets_test[0]\n",
    "# test_loader, datasets_test =  generateDataLoaderListing([sample_to_test], 1)\n",
    "\n",
    "# molecule_test = datasets_test[0]\n",
    "# smiles_processes = molecule_test.smiles\n",
    "# print(molecule_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with t = sample\n",
    "smiles_processes = mol_to_smiles(smiles_to_mol('COC(N)=O', with_atom_index=False))\n",
    "molecule_test = [constructGraph(smiles_processes, 0.74)]\n",
    "test_loader = DataLoader(molecule_test, batch_size=1, shuffle=True, follow_batch=['x_g', 'x_r'])\n",
    "molecule_test = molecule_test[0]\n",
    "print(molecule_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = Tester(args, args_test)\n",
    "# tester.test(test_loader, return_attention_weights=True)\n",
    "tester.test_single(test_loader, return_attention_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# att = tester.getAttention()\n",
    "att_mol = tester.getAttentionMol()\n",
    "if 'atom' in att_mol:\n",
    "    if len(args.reduced) >= 1:\n",
    "        sample_att = (att_mol['atom'], att_mol[args.reduced[0]])\n",
    "    else:\n",
    "        sample_att = (att_mol['atom'], None)\n",
    "else:\n",
    "    sample_att = (None, att_mol[args.reduced[0]])\n",
    "sample_graph = molecule_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attentions(args, sample_graph, sample_att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = smiles_processes\n",
    "mol = smiles_to_mol(smiles, with_atom_index=False)\n",
    "\n",
    "# reduced graph\n",
    "if args.schema in ['A']:\n",
    "    reduced_graph, cliques, edges = getReducedGraph(args, ['atom'], smiles, normalize=False)\n",
    "else:\n",
    "    reduced_graph, cliques, edges = getReducedGraph(args, args.reduced, smiles, normalize=False)\n",
    "\n",
    "sample_att_g, sample_att_r = sample_att\n",
    "if args.schema in ['A', 'R_N', 'AR', 'AR_0', 'AR_N']:\n",
    "    mask_graph_g = mask_graph(sample_att_g)\n",
    "if args.schema in ['R', 'R_0', 'R_N', 'AR', 'AR_0', 'AR_N']:\n",
    "        mask_graph_r = mask_reduced(sample_att_r)   \n",
    "if not args.schema in ['A']:\n",
    "    mask_graph_x = mask_rtog(smiles, cliques, mask_graph_r)\n",
    "    if args.schema in ['AR', 'AR_0', 'AR_N']:\n",
    "        mask_graph_x = mask_gandr(mask_graph_g, mask_graph_x)\n",
    "        display_interpret_weight(mol, None, None, mask_graph_x, None, scale=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset = datasets_graph\n",
    "print('Number of dataset:', len(all_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_embedding =  dict()\n",
    "feature_importance =  dict()\n",
    "\n",
    "count = 0 \n",
    "tester = Tester(args, args_test)\n",
    "\n",
    "b = False\n",
    "count_outlier = 0\n",
    "\n",
    "for d in tqdm(all_dataset):\n",
    "    # data loader\n",
    "    test_loader, datasets_test =  generateDataLoaderListing([all_dataset[d]], 1)\n",
    "    molecule_test = datasets_test[0]\n",
    "\n",
    "    # testing\n",
    "    # print(datasets_test)\n",
    "    try:\n",
    "        predicted = tester.test_single(test_loader, return_attention_weights=True, print_result=False)\n",
    "    except:\n",
    "        predicted = None\n",
    "\n",
    "    # if predicted != molecule_test.y:\n",
    "    #     print(molecule_test.smiles, \"TRUE:\", molecule_test.y, \"PREDICTED:\", predicted)\n",
    "\n",
    "    if predicted is not None:\n",
    "        # print(predicted)\n",
    "        try:\n",
    "            predicted = predicted.item()\n",
    "        except:\n",
    "            predicted = predicted[0][0]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        # embedding result\n",
    "        emb_mol = tester.getXEmbed()\n",
    "\n",
    "        # attention result\n",
    "        att_mol = tester.getAttentionMol()\n",
    "        sample_att = att_mol\n",
    "        sample_graph = molecule_test\n",
    "        if 'atom' in sample_att:\n",
    "            sample_att_g = sample_att['atom']\n",
    "        else:\n",
    "            sample_att_g = None\n",
    "        if len(args.reduced) != 0:\n",
    "            sample_att_r = sample_att[args.reduced[0]]\n",
    "        else:\n",
    "            sample_att_r = None\n",
    "        # sample_att_g, sample_att_r = sample_att\n",
    "        if args.schema in ['A', 'R_N', 'AR', 'AR_0', 'AR_N']:\n",
    "            mask_graph_g = mask_graph(sample_att_g)\n",
    "        if args.schema in ['R', 'R_0', 'R_N', 'AR', 'AR_0', 'AR_N']:\n",
    "            mask_graph_r = mask_reduced(sample_att_r)\n",
    "\n",
    "        # molecule\n",
    "        smiles = sample_graph.smiles\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "        if smiles not in feature_importance:\n",
    "            feature_embedding[smiles] = dict()\n",
    "            feature_importance[smiles] = dict()\n",
    "\n",
    "        # record importance\n",
    "        mask_graph_x = None\n",
    "\n",
    "        # if args.schema in ['A']:\n",
    "        if 'A' in args.schema:\n",
    "            mask_graph_x = mask_graph_g\n",
    "            # reduced graph\n",
    "            reduced_graph, cliques, edges = getReducedGraph(args, ['atom'], smiles, normalize=False)\n",
    "\n",
    "            # embedding\n",
    "            feature_embedding[smiles]['atom'] = emb_mol[0][:512]\n",
    "            # important\n",
    "            feature_importance[smiles]['atom'] = mask_graph_x['atom']\n",
    "\n",
    "        # elif args.schema in ['R', 'R_0', 'R_N', 'AR', 'AR_0', 'AR_N']:\n",
    "        if 'R' in args.schema:\n",
    "            mask_graph_x = mask_graph_r\n",
    "            # reduced graph\n",
    "            reduced_graph, cliques, edges = getReducedGraph(args, args.reduced, smiles, normalize=False)\n",
    "\n",
    "            if not args.schema in ['A']:\n",
    "                mask_graph_x = mask_rtog(smiles, cliques, mask_graph_r)\n",
    "                # if args.schema in ['AR', 'AR_0', 'AR_N']:\n",
    "                #     mask_graph_x = mask_gandr(mask_graph_g, mask_graph_x)\n",
    "\n",
    "            for i, r in enumerate(args.reduced):\n",
    "                # embedding \n",
    "                if 'A' in args.schema:\n",
    "                    feature_embedding[smiles][r] = emb_mol[0][256*(i+1):256*(i+2)]\n",
    "                else:\n",
    "                    feature_embedding[smiles][r] = emb_mol[0][256*i:256*(i+1)]\n",
    "                # important\n",
    "                feature_importance[smiles][r]= mask_graph_x['atom']\n",
    "    # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './dataset/'+args_test['log_folder_name']+'/embedding'+str(args_test['fold_number'])+'.pickle'\n",
    "with open(path, 'wb') as handle:\n",
    "    pickle.dump(feature_embedding, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './dataset/'+args_test['log_folder_name']+'/attention'+str(args_test['fold_number'])+'.pickle'\n",
    "with open(path, 'wb') as handle:\n",
    "    pickle.dump(feature_importance, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('myenv38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39c542c5c02b7a511e1ae79f0bb478cef0c12733a4ae7993722600208abb2b65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
